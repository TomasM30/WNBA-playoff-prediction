{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, recall_score, precision_score, f1_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set pandas options to display all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Set pandas options to display all rows\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "teams = pd.read_csv(\"../../datasets/teams.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INITIAL DATA CLEANING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Null Counts:\n",
      " year            0\n",
      "tmID            0\n",
      "franchID        0\n",
      "confID          0\n",
      "rank            0\n",
      "playoff        20\n",
      "firstRound     62\n",
      "semis         104\n",
      "finals        122\n",
      "name            0\n",
      "o_fgm           0\n",
      "o_fga           0\n",
      "o_ftm           0\n",
      "o_fta           0\n",
      "o_3pm           0\n",
      "o_3pa           0\n",
      "o_oreb          0\n",
      "o_dreb          0\n",
      "o_reb           0\n",
      "o_asts          0\n",
      "o_pf            0\n",
      "o_stl           0\n",
      "o_to            0\n",
      "o_blk           0\n",
      "o_pts           0\n",
      "d_fgm           0\n",
      "d_fga           0\n",
      "d_ftm           0\n",
      "d_fta           0\n",
      "d_3pm           0\n",
      "d_3pa           0\n",
      "d_oreb          0\n",
      "d_dreb          0\n",
      "d_reb           0\n",
      "d_asts          0\n",
      "d_pf            0\n",
      "d_stl           0\n",
      "d_to            0\n",
      "d_blk           0\n",
      "d_pts           0\n",
      "won             0\n",
      "lost            0\n",
      "GP              0\n",
      "homeW           0\n",
      "homeL           0\n",
      "awayW           0\n",
      "awayL           0\n",
      "confW           0\n",
      "confL           0\n",
      "min             0\n",
      "attend          0\n",
      "arena           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Converting Year to int\n",
    "teams['year'] = teams['year'].astype(int)\n",
    "\n",
    "# Mapping playoff column to 0 and 1\n",
    "teams['playoff'] = teams['playoff'].map({'Y': 1, 'N': 0})\n",
    "\n",
    "# Now we need to shift the playoff column from a year to the next so the playoff is predicted by the year before\n",
    "teams.sort_values(['franchID', 'year'])\n",
    "teams['playoff'] = teams.groupby('franchID')['playoff'].shift(-1)\n",
    "teams.loc[teams['franchID'] != teams['franchID'].shift(-1), 'playoff'] = np.nan\n",
    "\n",
    "\n",
    "# Checking if there are collumns with all rows the same and dropping them\n",
    "cols_with_same_values = []\n",
    "for col in teams.columns:\n",
    "    if len(teams[col].unique()) == 1:\n",
    "        cols_with_same_values.append(col)\n",
    "\n",
    "teams = teams.drop(cols_with_same_values, axis=1)\n",
    "\n",
    "# Check the number of nulls in each column\n",
    "null_counts = teams.isnull().sum()\n",
    "print(\"\\nNull Counts:\\n\", null_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the columns `firstRound`, `semis` and `finals` are the ones that have null values and can take the values of 'W', 'L, or NaN we need to find a way to fix this. Since the most important part for our prediction is the regular season performance we could just remove those columns. However, we decided to keep them and fill the NaN values with -1, the 'W' values with 1 and the 'L' values with 0. This way we can use these columns as features for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom function to replace values\n",
    "def replace_values(value):\n",
    "    if pd.isna(value):\n",
    "        return -1\n",
    "    elif value == 'W':\n",
    "        return 1\n",
    "    elif value == 'L':\n",
    "        return 0\n",
    "    return value\n",
    "\n",
    "# Apply the custom function to the columns\n",
    "teams['firstRound'] = teams['firstRound'].apply(replace_values)\n",
    "teams['semis'] = teams['semis'].apply(replace_values)\n",
    "teams['finals'] = teams['finals'].apply(replace_values)\n",
    "\n",
    "teams.sort_values(by=['franchID','year'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to find a way to treat categorical features such as `name` and `arena`. For that, we will check the number of unique values for each of the categorical features and decide what strategy to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmID        20\n",
      "franchID    18\n",
      "confID       2\n",
      "name        20\n",
      "arena       22\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Select categorical columns\n",
    "categorical_columns = teams.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Get the number of unique values for each categorical column\n",
    "unique_values = teams[categorical_columns].nunique()\n",
    "\n",
    "print(unique_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the number of unique values is too high and the columns generated on one-hot encoding would be too many, we decided to use `Sckit-learn`'s `LabelEncoder` to transform the categorical features into numerical ones as there is no ordinal relationship between the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     tmID  franchID  confID  name  arena\n",
      "0       0         0       0     0     11\n",
      "1       0         0       0     0     11\n",
      "2       1         1       0     1      4\n",
      "3       1         1       0     1      4\n",
      "4       1         1       0     1      4\n",
      "5       1         1       0     1      4\n",
      "6       1         1       0     1      4\n",
      "7       1         1       0     1      4\n",
      "8       1         1       0     1     17\n",
      "9       2         2       0     2     19\n",
      "10      2         2       0     2     19\n",
      "11      2         2       0     2     19\n",
      "12      2         2       0     2     19\n",
      "13      3         3       0     3     12\n",
      "14      3         3       0     3     12\n",
      "15      3         3       0     3     12\n",
      "16      3         3       0     3     12\n",
      "86     12         4       0    12      3\n",
      "87     12         4       0    12      3\n",
      "88     12         4       0    12      3\n",
      "17      4         4       0     4     10\n",
      "18      4         4       0     4     10\n",
      "19      4         4       0     4     10\n",
      "20      4         4       0     4     10\n",
      "21      4         4       0     4     10\n",
      "22      4         4       0     4     10\n",
      "23      4         4       0     4     10\n",
      "24      5         5       0     5     16\n",
      "25      5         5       0     5     16\n",
      "26      5         5       0     5     16\n",
      "27      5         5       0     5     16\n",
      "28      5         5       0     5     16\n",
      "29      5         5       0     5     16\n",
      "30      5         5       0     5     16\n",
      "31      5         5       0     5     16\n",
      "32      5         5       0     5     16\n",
      "33      5         5       0     5     16\n",
      "34      6         6       1     6      5\n",
      "35      6         6       1     6      5\n",
      "36      6         6       1     6      5\n",
      "37      6         6       1     6      5\n",
      "38      6         6       1     6     18\n",
      "39      6         6       1     6     18\n",
      "40      6         6       1     6     18\n",
      "41      6         6       1     6     18\n",
      "42      6         6       1     6     18\n",
      "43      7         7       0     7      6\n",
      "44      7         7       0     7      6\n",
      "45      7         7       0     7      6\n",
      "46      7         7       0     7      6\n",
      "47      7         7       0     7      6\n",
      "48      7         7       0     7      6\n",
      "49      7         7       0     7      6\n",
      "50      7         7       0     7      6\n",
      "51      7         7       0     7      6\n",
      "52      7         7       0     7      6\n",
      "53      8         8       1     8     14\n",
      "54      8         8       1     8     14\n",
      "55      8         8       1     8     14\n",
      "56      8         8       1     8     14\n",
      "57      8         8       1     8     14\n",
      "58      8         8       1     8     14\n",
      "59      8         8       1     8     14\n",
      "60      8         8       1     8     14\n",
      "61      8         8       1     8     14\n",
      "62      8         8       1     8     14\n",
      "63      9         9       0     9      2\n",
      "64      9         9       0     9      2\n",
      "65      9         9       0     9      2\n",
      "66     10        10       1    10     15\n",
      "67     10        10       1    10     15\n",
      "68     10        10       1    10     15\n",
      "69     10        10       1    10     15\n",
      "70     10        10       1    10     15\n",
      "71     10        10       1    10     15\n",
      "72     10        10       1    10     15\n",
      "73     10        10       1    10     15\n",
      "74     10        10       1    10     15\n",
      "75     10        10       1    10     15\n",
      "76     11        11       0    11      9\n",
      "77     11        11       0    11      9\n",
      "78     11        11       0    11      9\n",
      "79     11        11       0    11      9\n",
      "80     11        11       0    11      9\n",
      "81     11        11       0    11      9\n",
      "82     11        11       0    11      9\n",
      "83     11        11       0    11      9\n",
      "84     11        11       0    11      9\n",
      "85     11        11       0    11      9\n",
      "89     13        12       1    13     20\n",
      "90     13        12       1    13     20\n",
      "91     13        12       1    13     20\n",
      "92     13        12       1    13     20\n",
      "93     13        12       1    13     20\n",
      "94     13        12       1    13     20\n",
      "95     13        12       1    13     20\n",
      "96     13        12       1    13     20\n",
      "97     13        12       1    13     20\n",
      "98     13        12       1    13     20\n",
      "99     14        13       1    14     13\n",
      "100    14        13       1    14     13\n",
      "101    14        13       1    14     13\n",
      "102    15        14       1    15      0\n",
      "103    15        14       1    15      0\n",
      "104    15        14       1    15      0\n",
      "105    15        14       1    15      0\n",
      "106    15        14       1    15      0\n",
      "107    15        14       1    15      0\n",
      "108    15        14       1    15      0\n",
      "109    15        14       1    15      0\n",
      "110    15        14       1    15      0\n",
      "111    15        14       1    15      0\n",
      "129    18        15       1    18      7\n",
      "130    18        15       1    18      7\n",
      "131    18        15       1    18      7\n",
      "112    16        15       1    16      1\n",
      "113    16        15       1    16      1\n",
      "114    16        15       1    16      1\n",
      "115    16        15       1    16      1\n",
      "116    16        15       1    16      1\n",
      "117    16        15       1    16      1\n",
      "118    16        15       1    16      1\n",
      "119    17        16       1    17      8\n",
      "120    17        16       1    17      8\n",
      "121    17        16       1    17      8\n",
      "122    17        16       1    17      8\n",
      "123    17        16       1    17      8\n",
      "124    17        16       1    17      8\n",
      "125    17        16       1    17      8\n",
      "126    17        16       1    17      8\n",
      "127    17        16       1    17      8\n",
      "128    17        16       1    17      8\n",
      "132    19        17       0    19     21\n",
      "133    19        17       0    19     21\n",
      "134    19        17       0    19     21\n",
      "135    19        17       0    19     21\n",
      "136    19        17       0    19     21\n",
      "137    19        17       0    19     21\n",
      "138    19        17       0    19     21\n",
      "139    19        17       0    19     21\n",
      "140    19        17       0    19     21\n",
      "141    19        17       0    19     21\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "for col in categorical_columns:\n",
    "    teams[col] = le.fit_transform(teams[col])\n",
    "\n",
    "categorical_df = teams[categorical_columns]\n",
    "print(categorical_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for statistical features, we decided to create new features that represent the team's performance in the regular season. We will create the following features:\n",
    "\n",
    "- win_rate: the ratio of games won to games played;\n",
    "- points_diff: the difference between the points scored and the points conceded;\n",
    "- poss: the number of possessions;\n",
    "- off_rating: the number of points scored per 100 possessions;\n",
    "- def_rating: the number of points conceded per 100 possessions;\n",
    "- margin: the difference between the offensive and defensive ratings;\n",
    "- net_rating: an estimate of the team's point differential per 100 possessions;\n",
    "- pace: the number of possessions per 40 minutes;\n",
    "- ts_pct: the true shooting percentage - A measure of shooting efficiency that takes into account 2-point field goals, 3-point field goals, and free throws;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['year', 'tmID', 'franchID', 'confID', 'rank', 'playoff', 'firstRound',\n",
      "       'semis', 'finals', 'name', 'o_fgm', 'o_fga', 'o_ftm', 'o_fta', 'o_3pm',\n",
      "       'o_3pa', 'o_oreb', 'o_dreb', 'o_reb', 'o_asts', 'o_pf', 'o_stl', 'o_to',\n",
      "       'o_blk', 'o_pts', 'd_fgm', 'd_fga', 'd_ftm', 'd_fta', 'd_3pm', 'd_3pa',\n",
      "       'd_oreb', 'd_dreb', 'd_reb', 'd_asts', 'd_pf', 'd_stl', 'd_to', 'd_blk',\n",
      "       'd_pts', 'won', 'lost', 'GP', 'homeW', 'homeL', 'awayW', 'awayL',\n",
      "       'confW', 'confL', 'min', 'attend', 'arena', 'win_rate', 'point_diff',\n",
      "       'poss', 'off_rating', 'def_rating', 'margin', 'net_rating', 'pace',\n",
      "       'TS'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "teams['win_rate'] = teams['won'] / (teams['won'] + teams['lost'])\n",
    "teams['point_diff'] = teams['o_pts'] - teams['d_pts']\n",
    "teams['Tm_Poss'] = (teams['o_fga'] + 0.4 * teams['o_fta'] - \n",
    "                    1.07 * (teams['o_oreb'] / (teams['o_oreb'] + teams['d_dreb'])) * \n",
    "                    (teams['o_fga'] - teams['o_fgm']) + teams['o_to'])\n",
    "\n",
    "teams['Opp_Poss'] = (teams['d_fga'] + 0.4 * teams['d_fta'] - \n",
    "                     1.07 * (teams['d_oreb'] / (teams['d_oreb'] + teams['o_dreb'])) * \n",
    "                     (teams['d_fga'] - teams['d_fgm']) + teams['d_to'])\n",
    "\n",
    "teams['poss'] = 0.5 * (teams['Tm_Poss'] + teams['Opp_Poss'])\n",
    "\n",
    "teams['off_rating'] = (teams['o_pts'] / teams['poss']) * 100\n",
    "\n",
    "# Calculate defensive rating\n",
    "teams['def_rating'] = (teams['d_pts'] / teams['poss']) * 100\n",
    "\n",
    "# Calculate margin\n",
    "teams['margin'] = teams['off_rating'] - teams['def_rating']\n",
    "\n",
    "# Calculate net rating\n",
    "teams['net_rating'] = (teams['point_diff'] / teams['poss']) * 100\n",
    "\n",
    "# Calculate the pace\n",
    "teams['pace'] = 40 * ((teams['Tm_Poss'] + teams['Opp_Poss']) / (2 * (teams['min'] / 5)))\n",
    "\n",
    "\n",
    "# Calculate True Shooting Attempts (TSA)\n",
    "teams['TSA'] = teams['o_fga'] + 0.44 * teams['o_fta']\n",
    "\n",
    "# Calculate True Shooting Percentage (TS%)\n",
    "teams['TS'] = teams['o_pts'] / (2 * teams['TSA'])\n",
    "\n",
    "# Drop intermediate columns\n",
    "teams = teams.drop(columns=['Tm_Poss', 'Opp_Poss', 'TSA'])\n",
    "\n",
    "print(teams.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Accuracy: 0.62\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Log Loss</th>\n",
       "      <th>ROC AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Bernoulli Naive Bayes</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.838150</td>\n",
       "      <td>0.644444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.681394</td>\n",
       "      <td>0.469444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.589744</td>\n",
       "      <td>0.749468</td>\n",
       "      <td>0.591667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Gaussian Naive Bayes</td>\n",
       "      <td>0.589744</td>\n",
       "      <td>10.207099</td>\n",
       "      <td>0.519444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.564103</td>\n",
       "      <td>0.800564</td>\n",
       "      <td>0.462500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>0.564103</td>\n",
       "      <td>1.537005</td>\n",
       "      <td>0.529167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>16.635532</td>\n",
       "      <td>0.462500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Extra Trees</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.781579</td>\n",
       "      <td>0.594444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MLP</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>24.953299</td>\n",
       "      <td>0.255556</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Model  Accuracy   Log Loss   ROC AUC\n",
       "7  Bernoulli Naive Bayes  0.666667   0.838150  0.644444\n",
       "5                    SVM  0.615385   0.681394  0.469444\n",
       "1          Random Forest  0.589744   0.749468  0.591667\n",
       "6   Gaussian Naive Bayes  0.589744  10.207099  0.519444\n",
       "4                    KNN  0.564103   0.800564  0.462500\n",
       "8      Gradient Boosting  0.564103   1.537005  0.529167\n",
       "0          Decision Tree  0.538462  16.635532  0.462500\n",
       "2            Extra Trees  0.538462   0.781579  0.594444\n",
       "3                    MLP  0.307692  24.953299  0.255556"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Cross-Validation Accuracy: 0.54 ± 0.11\n",
      "Random Forest Cross-Validation Accuracy: 0.63 ± 0.06\n",
      "Extra Trees Cross-Validation Accuracy: 0.61 ± 0.09\n",
      "MLP Cross-Validation Accuracy: 0.46 ± 0.05\n",
      "KNN Cross-Validation Accuracy: 0.57 ± 0.05\n",
      "SVM Cross-Validation Accuracy: 0.57 ± 0.02\n",
      "Gaussian Naive Bayes Cross-Validation Accuracy: 0.64 ± 0.08\n",
      "Bernoulli Naive Bayes Cross-Validation Accuracy: 0.63 ± 0.08\n",
      "Gradient Boosting Cross-Validation Accuracy: 0.53 ± 0.14\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, log_loss, roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Assuming 'teams' DataFrame is already loaded and preprocessed\n",
    "\n",
    "# Copy and preprocess data\n",
    "proc_data = teams.copy()\n",
    "proc_data = proc_data.drop('franchID', axis=1)\n",
    "proc_data = proc_data.dropna()\n",
    "\n",
    "# Define the testing year\n",
    "testing_year = 6\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "x_train = proc_data.loc[proc_data['year'] <= testing_year].drop('playoff', axis=1)\n",
    "y_train = proc_data.loc[proc_data['year'] <= testing_year]['playoff'].astype('int')\n",
    "x_test = proc_data.loc[proc_data['year'] > testing_year].drop('playoff', axis=1)\n",
    "y_test = proc_data.loc[proc_data['year'] > testing_year]['playoff'].astype('int')\n",
    "\n",
    "# Define models\n",
    "models = [\n",
    "    ('Decision Tree', DecisionTreeClassifier()),\n",
    "    ('Random Forest', RandomForestClassifier()),\n",
    "    ('Extra Trees', ExtraTreesClassifier()),\n",
    "    ('MLP', MLPClassifier(random_state=184, max_iter=1000)),\n",
    "    ('KNN', KNeighborsClassifier()),\n",
    "    ('SVM', SVC(probability=True, random_state=42)),  # Enable probability estimates for SVM\n",
    "    ('Gaussian Naive Bayes', GaussianNB()),\n",
    "    ('Bernoulli Naive Bayes', BernoulliNB()),\n",
    "    ('Gradient Boosting', GradientBoostingClassifier(random_state=42))\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "# Train and evaluate models\n",
    "for model_name, model in models:\n",
    "    model.fit(x_train, y_train)\n",
    "    y_pred = model.predict(x_test)\n",
    "    y_proba = model.predict_proba(x_test)[:, 1]  # Get probability estimates for the positive class\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    logloss = log_loss(y_test, y_proba)\n",
    "    roc_auc = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "    results.append((model_name, accuracy, logloss, roc_auc))\n",
    "\n",
    "# Calculate baseline accuracy\n",
    "baseline_accuracy = max(y_test.mean(), 1 - y_test.mean())\n",
    "print(f\"Baseline Accuracy: {baseline_accuracy:.2f}\")\n",
    "\n",
    "# Display results\n",
    "results_df = pd.DataFrame(results, columns=['Model', 'Accuracy', 'Log Loss', 'ROC AUC'])\n",
    "results_df = results_df.sort_values('Accuracy', ascending=False)\n",
    "display(results_df)\n",
    "\n",
    "# Use cross-validation to get a more robust estimate of model performance\n",
    "for model_name, model in models:\n",
    "    cv_scores = cross_val_score(model, x_train, y_train, cv=5, scoring='accuracy')\n",
    "    print(f\"{model_name} Cross-Validation Accuracy: {cv_scores.mean():.2f} ± {cv_scores.std():.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
